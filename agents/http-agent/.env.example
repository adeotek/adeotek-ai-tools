# Intelligent HTTP Agent Environment Variables
# Copy this file to .env and configure your settings

# ===== LLM Configuration =====
# Choose one provider and configure accordingly

# --- OpenAI ---
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-your-api-key-here
LLM_MODEL=gpt-4-turbo-preview

# --- Anthropic Claude ---
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-your-api-key-here
# LLM_MODEL=claude-3-5-sonnet-20241022

# --- Google Gemini ---
# LLM_PROVIDER=gemini
# GEMINI_API_KEY=AIza-your-api-key-here
# LLM_MODEL=gemini-1.5-pro

# --- Ollama (Local LLM) ---
# LLM_PROVIDER=ollama
# LLM_MODEL=llama2
# HTTP_AGENT_LLM_BASE_URL=http://localhost:11434
# Note: Make sure Ollama is running with: ollama serve
# Pull model with: ollama pull llama2

# --- LM Studio (Local LLM) ---
# LLM_PROVIDER=lmstudio
# LLM_MODEL=local-model
# HTTP_AGENT_LLM_BASE_URL=http://localhost:1234
# Note: Make sure LM Studio server is running

# ===== Server Configuration =====
PORT=8080

# ===== HTTP Client Configuration =====
HTTP_TIMEOUT=30
VERIFY_SSL=true
BLOCK_PRIVATE_IPS=true
